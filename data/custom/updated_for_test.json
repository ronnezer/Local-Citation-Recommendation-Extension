{
    "W11-1305": {
        "title": "Shared task system description:\nFrustratingly hard compositionality prediction",
        "abstract": "We considered a wide range of features for the DiSCo 2011 shared task about compositionality prediction for word pairs, including COALS-based endocentricity scores, compositionality scores based on distributional clusters, statistics about wordnet-induced paraphrases, hyphenation, and the likelihood of long translation equivalents in other languages. Many of the features we considered correlated significantly with human compositionality scores, but in support vector regression experiments we obtained the best results using only COALS-based endocentricity scores. Our system was nevertheless the best performing system in the shared task, and average error reductions over a simple baseline in cross-validation were 13.7% for English and 50.1% for German.",
        "year": 2011
    },
    "W95-0110": {
        "title": "Inverse Document Frequency (IDF):\nA Measure of Deviations from Poisson",
        "abstract": "Low frequency words tend to be rich in content, and vice versa. But not all equally frequent words are equally meaningful. We will use inverse document frequency (IDF), a quantity borrowed from Information Retrieval, to distinguish words like somewhat and boycott. Both somewhat and boycott appeared approximately 1000 times in a corpus of 1989 Associated Press articles, but boycott is a better keyword because its IDF is farther from what would be expected by chance (Poisson). Word frequency is commonly used in all sorts of natural language applications. The practice implicitly assumes that words (and ngrams) are distributed by a single parameter distribution such as a Poisson or a Binomial. But we find that these distributions do not fit the data very well. Both the Poisson and Binomial assume that the variance over documents is no larger than the mean, and yet, we find that it can be quite a bit larger, especially for interesting words such as boycott where there are hidden variables such as topic that conspire to undermine the independence assumption behind the Poisson and the Binomial. Much better fits are obtained by introducing a second parameter such as inverse document frequency (IDF). Inverse document frequency (IDF) is commonly used in Information Retrieval (Sparck Jones, 1972). IDF is defined as \u2014log2df/D, where D is the number of documents in the collection and df w is the document frequency, the number of documents that contain w. Obviously, there is a strong relationship between document frequency, df, and word frequency, f. The relationship is shown in Figure 1, a plot of log of and IDF for 193 words selected from a 50 million word corpus of 1989 Associated Press (AP) Newswire stories (D = 85,432 stories). Although logiof,\u201e is highly correlated with IDF (p = \u2014 0.994), it would be a mistake to assume that the two variables are completely predictable from one another. Indeed, the experience of the Information Retrieval community has indicated that IDF is a very useful quantity. Attempts to replace IDF with fw (or some simple transform off) have not been very successful. Figure 2 shows one such attempt. It compares the observed IDF with IDF, an estimate based on f Assume that a document is merely a &quot;bag of words&quot; with no interesting structure (content). Words are randomly generated by a Poisson process, it. The probability of k instances of a word w is ic(9,k) fw where 0= In particular, the probability that w will not be found in a document is rc(0,0). Conversely, the probability of at least one w is 1 \u2014 TC(0, 0). And therefore, IDF ought to be: The prediction errors are shown in more detail in Figure 3, which plots the residual IDF (difference between predicted and observed) as a function of log iof,\u201e for the same 193 words shown in Figure 2. The prediction errors are relatively large in the middle of the frequency range, and smaller at both ends. Unfortunately, we believe the words in the middle are often the most important words for Information Retrieval purposes.",
        "year": 1995
    },
    "W09-0510": {
        "title": "Incrementality, Speaker-Hearer Switching\nand the Disambiguation Challenge",
        "abstract": "Taking so-called split utterances as our point of departure, we argue that a new perspective on the major challenge of disambiguation becomes available, given a framework in which both parsing and generation incrementally involve the same mechanisms for constructing trees reflecting interpretation (Dynamic Syntax: (Cann et al., 2005; Kempson et al., 2001)). With all dependencies, syntactic, semantic and pragmatic, defined in terms of incremental progressive tree growth, the phenomenon of speaker/hearer role-switch emerges as an immediate consequence, with the potential for clarification, acknowledgement, correction, all available incrementally at any sub-sentential point in the interpretation process. Accordingly, at all intermediate points where interpretation of an utterance subpart is not fully determined for the hearer in context, uncertainty can be resolved immediately by suitable clarification/correction/repair/extension as an exchange between interlocutors. The result is a major check on the combinatorial explosion of alternative structures and interpretations at each choice point, and the basis for a model of how interpretation in context can be established without either party having to make assumptions about what information they and their interlocutor share in resolving ambiguities.",
        "year": 2009
    },
    "W03-1006": {
        "title": "Use of Deep Linguistic Features for the Recognition and Labeling of\nSemantic Arguments",
        "abstract": "We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features. We also show that predicting labels from a \u201clightweight\u201d parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features.",
        "year": 2003
    },
    "W03-1007": {
        "title": "Maximum Entropy Models for FrameNet Classification",
        "abstract": "The development of FrameNet, a large database of semantically annotated sentences, has primed research into statistical methods for semantic tagging. We advance previous work by adopting a Maximum Entropy approach and by using previous tag information to find the highest probability tag sequence for a given sentence. Further we examine the use of sentence level syntactic pattern features to increase performance. We analyze our strategy on both human annotated and automatically identified frame elements, and compare performance to previous work on identical test data. Experiments indicate a statistically significant improvement (p<0.01) of over 6%.",
        "year": 2003
    },
    "S14-2078": {
        "title": "NTNU: Measuring Semantic Similarity with\nSublexical Feature Representations and Soft Cardinality",
        "abstract": "The paper describes the approaches taken by the NTNU team to the SemEval 2014 Semantic Textual Similarity shared task. The solutions combine measures based on lexical soft cardinality and character n-gram feature representations with lexical distance metrics from TakeLab\u2019s baseline system. The final NTNU system is based on bagged support vector machine regression over the datasets from previous shared tasks and shows highly competitive performance, being the best system on three of the datasets and third best overall (on weighted mean over all six datasets).",
        "year": 2014
    },
    "S14-2079": {
        "title": "OPI: Semeval-2014 Task 3 System Description",
        "abstract": "In this paper, we describe the OPI system participating in the Semeval-2014 task 3 Cross-Level Semantic Similarity. Our approach is knowledge-poor, there is no exploitation of any structured knowledge resources as Wikipedia, WordNet or BabelNet. The method is also fully unsupervised, the training set is only used in order to tune the system. System measures the semantic similarity of texts using corpusbased measures of termsets similarity.",
        "year": 2014
    },
    "W03-1002": {
        "title": "Statistical Machine Translation Using Coercive Two-Level Syntactic\nTransduction",
        "abstract": "We define, implement and evaluate a novel model for statistical machine translation, which is based on shallow syntactic analysis (part-of-speech tagging and phrase chunking) in both the source and target languages. It is able to model long-distance constituent motion and other syntactic phenomena without requiring a full parse in either language. We also examine aspects of lexical transfer, suggesting and exploring a concept of translation coercion across parts of speech, as well as a transfer model based on lemma-to-lemma translation probabilities, which holds promise for improving machine translation of low-density languages. Experiments are performed in both Arabic-to-English and French-to-English translation demonstrating the efficacy of the proposed techniques. Performance is automatically evaluated via the Bleu score metric.",
        "year": 2003
    },
    "W03-1003": {
        "title": "Cross-Lingual Lexical Triggers in Statistical Language Modeling",
        "abstract": "We propose new methods to take advantage of text in resource-rich languages to sharpen statistical language models in resource-deficient languages. We achieve this through an extension of the method of lexical triggers to the cross-language problem, and by developing a likelihoodbased adaptation scheme for combining a trigger model with an -gram model. We describe the application of such language models for automatic speech recognition. By exploiting a side-corpus of contemporaneous English news articles for adapting a static Chinese language model to transcribe Mandarin news stories, we demonstrate significant reductions in both perplexity and recognition errors. We also compare our cross-lingual adaptation scheme to monolingual language model adaptation, and to an alternate method for exploiting cross-lingual cues, via crosslingual information retrieval and machine translation, proposed elsewhere.",
        "year": 2003
    },
    "W03-1001": {
        "title": "A Projection Extension Algorithm for Statistical Machine Translation",
        "abstract": "In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrasebased models. The units of translation are blocks \u2013 pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying high-precision word alignment. The system performance is significantly increased by applying a novel block extension algorithm using an additional highrecall word alignment. The blocks are further filtered using unigram-count selection criteria. The system has been successfully test on a Chinese-English and an ArabicEnglish translation task.",
        "year": 2003
    },
    "S14-2072": {
        "title": "Meerkat Mafia: Multilingual and Cross-Level\nSemantic Textual Similarity Systems",
        "abstract": "We describe UMBC\u2019s systems developed for the SemEval 2014 tasks on Multilingual Semantic Textual Similarity (Task 10) and Cross-Level Semantic Similarity (Task 3). Our best submission in the Multilingual task ranked second in both English and Spanish subtasks using an unsupervised approach. Our best systems for Cross-Level task ranked second in Paragraph-Sentence and first in both Sentence-Phrase and Word-Sense subtask. The system ranked first for the PhraseWord subtask but was not included in the official results due to a late submission.",
        "year": 2014
    },
    "S14-2073": {
        "title": "MindLab-UNAL: Comparing Metamap and T-mapper for Medical\nConcept Extraction in SemEval 2014 Task 7",
        "abstract": "This paper describes our participation in task 7 of SemEval 2014, which focuses on analysis of clinical text. The task is divided into two parts: recognizing mentions of concepts that belong to the UMLS (Unified Medical Language System) semantic group disorders, and mapping each disorder to a unique UMLS CUI (Concept Unique Identifier), if possible. For identifying and mapping disorders belonging to the UMLS meta thesaurus, we explore two tools: Metamap and T-mapper. Additionally, a Named Entity Recognition system, based on a maximum entropy model, was implemented to identify other disorders.",
        "year": 2014
    },
    "S14-2070": {
        "title": "LT3: Sentiment Classification in User-Generated Content Using a Rich\nFeature Set",
        "abstract": "This paper describes our contribution to the SemEval-2014 Task 9 on sentiment analysis in Twitter. We participated in both strands of the task, viz. classification at message-level (subtask B), and polarity disambiguation of particular text spans within a message (subtask A). Our experiments with a variety of lexical and syntactic features show that our systems benefit from rich feature sets for sentiment analysis on user-generated content. Our systems ranked ninth among 27 and sixteenth among 50 submissions for task A and B respectively.",
        "year": 2014
    },
    "S14-2071": {
        "title": "LyS: Porting a Twitter Sentiment Analysis Approach\nfrom Spanish to English",
        "abstract": "This paper proposes an approach to solve message- and phrase-level polarity classification in Twitter, derived from an existing system designed for Spanish. As a first step, an ad-hoc preprocessing is performed. We then identify lexical, psychological and semantic features in order to capture different dimensions of the human language which are helpful to detect sentiment. These features are used to feed a supervised classifier after applying an information gain filter, to discriminate irrelevant features. The system is evaluated on the SemEval 2014 task 9: Sentiment Analysis in Twitter. Our approach worked competitively both in message- and phraselevel tasks. The results confirm the robustness of the approach, which performed well on different domains involving short informal texts.",
        "year": 2014
    },
    "S14-2076": {
        "title": "NRC-Canada-2014: Detecting Aspects and Sentiment\nin Customer Reviews",
        "abstract": "Reviews depict sentiments of customers towards various aspects of a product or service. Some of these aspects can be grouped into coarser aspect categories. SemEval-2014 had a shared task (Task 4) on aspect-level sentiment analysis, with over 30 teams participated. In this paper, we describe our submissions, which stood first in detecting aspect categories, first in detecting sentiment towards aspect categories, third in detecting aspect terms, and first and second in detecting sentiment towards aspect terms in the laptop and restaurant domains, respectively.",
        "year": 2014
    },
    "S14-2077": {
        "title": "NRC-Canada-2014: Recent Improvements in\nthe Sentiment Analysis of Tweets",
        "abstract": "This paper describes state-of-the-art statistical systems for automatic sentiment analysis of tweets. In a Semeval-2014 shared task (Task 9), our submissions obtained highest scores in the term-level sentiment classification subtask on both the 2013 and 2014 tweets test sets. In the message-level sentiment classification task, our submissions obtained highest scores on the LiveJournal blog posts test set, sarcastic tweets test set, and the 2013 SMS test set. These systems build on our SemEval-2013 sentiment analysis systems (Mohammad et al., 2013) which ranked first in both the termand message-level subtasks in 2013. Key improvements over the 2013 systems are in the handling of negation. We create separate tweet-specific sentiment lexicons for terms in affirmative contexts and in negated contexts.",
        "year": 2014
    },
    "S14-2074": {
        "title": "NILC USP: An Improved Hybrid System for Sentiment Analysis in\nTwitter Messages",
        "abstract": "This paper describes the NILC USP system that participated in SemEval-2014 Task 9: Sentiment Analysis in Twitter, a re-run of the SemEval 2013 task under the same name. Our system is an improved version of the system that participated in the 2013 task. This system adopts a hybrid classification process that uses three classification approaches: rule-based, lexiconbased and machine learning. We suggest a pipeline architecture that extracts the best characteristics from each classifier. In this work, we want to verify how this hybrid approach would improve with better classifiers. The improved system achieved an F-score of 65.39% in the Twitter message-level subtask for 2013 dataset (+ 9.08% of improvement) and 63.94% for 2014 dataset.",
        "year": 2014
    },
    "S14-2075": {
        "title": "NILC USP: Aspect Extraction using Semantic Labels",
        "abstract": "This paper details the system NILC USP that participated in the Semeval 2014: Aspect Based Sentiment Analysis task. This system uses a Conditional Random Field (CRF) algorithm for extracting the aspects mentioned in the text. Our work added semantic labels into a basic feature set for measuring the efficiency of those for aspect extraction. We used the semantic roles and the highest verb frame as features for the machine learning. Overall, our results demonstrated that the system could not improve with the use of this semantic information, but its precision was increased.",
        "year": 2014
    },
    "W10-4159": {
        "title": "Research of People Disambiguation by Combining\nMultiple knowledges",
        "abstract": "With the rapid development of Internet and many related technology, Web has become the main source of information. For many search engines, there are many different identities in the returned results of character information query. Thus the Research of People disambiguation is important. In this paper we attempt to solve this problem by combing different knowledge. As people usually have different kind of careers, so we first utilize this knowledge to classify people roughly. Then we use social context of people to identify different person. The experimental results show that these knowledge are helpful for people disambiguation.",
        "year": 2010
    },
    "S13-2008": {
        "title": "HsH: Estimating Semantic Similarity of Words and Short Phrases with\nFrequency Normalized Distance Measures",
        "abstract": "This paper describes the approach of the Hochschule Hannover to the SemEval 2013 Task Evaluating Phrasal Semantics. In order to compare a single word with a two word phrase we compute various distributional similarities, among which a new similarity measure, based on Jensen-Shannon Divergence with a correction for frequency effects. The classification is done by a support vector machine that uses all similarities as features. The approach turned out to be the most successful one in the task.",
        "year": 2013
    },
    "S12-1063": {
        "title": "UTDHLT: COPACETIC System for Choosing Plausible Alternatives",
        "abstract": "The Choice of Plausible Alternatives (COPA) task in SemEval-2012 presents a series of forced-choice questions wherein each question provides a premise and two viable cause or effect scenarios. The correct answer is the cause or effect that is the most plausible. This paper describes the COPACETIC system developed by the University of Texas at Dallas (UTD) for this task. We approach this task by casting it as a classification problem and using features derived from bigram co-occurrences, TimeML temporal links between events, single-word polarities from the Harvard General Inquirer, and causal syntactic dependency structures within the gigaword corpus. Additionally, we show that although each of these components improves our score for this evaluation, the difference in accuracy between using all of these features and using bigram co-occurrence information alone is not statistically significant.",
        "year": 2012
    }}